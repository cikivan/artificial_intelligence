{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Linear Regression with PyTorch\n",
    "#### 1. About Linear Regression\n",
    "##### 1.1 Simple Linear Regression Basics\n",
    "- Allows us to <b> understand relationship between two continuous variables </b>\n",
    "- Example\n",
    "    - x: independent variable <- when X changes it affects Y (when a,b are constant)\n",
    "        - weigth\n",
    "    - y: dependent variable\n",
    "        - height\n",
    "    - $ y = \\alpha x + \\beta $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Example of simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Modeling behaviour between x y\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1) # seed\n",
    "n = 50 # number of points\n",
    "x = np.random.randn(n) # x variable\n",
    "y = x * np.random.randn(n) # y variable\n",
    "\n",
    "# We try to make there a line that will fit the so much points as it is possible\n",
    "\n",
    "colors = np.random.rand(n)\n",
    "plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "\n",
    "plt.scatter(x, y, c=colors, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3 Aim of Linear Regression\n",
    "- Minimize thedistance between the pointsandthe line $ y = \\alpha x + \\beta $\n",
    "- Adjusting \n",
    "    - Coefficient: $ \\alpha $ <- changes orientation of the line\n",
    "    - Bias/Intercept: $ \\beta $ <- moves the line up or down\n",
    "\n",
    "#### 2. Building a Linear Regression Model with PyTorch\n",
    "##### 2.1 Example\n",
    "- Coefficient: $ \\alpha = 1 $ , when alfa is large number then the line is very steep, or when it is close to 0 it is similar to straight line\n",
    "- Bias/Intercept: $ \\beta = 1 $\n",
    "- Equation: $ y = 2x + 1 $\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Building a Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_values = [i for i in range(11)]\n",
    "x_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n"
     ]
    }
   ],
   "source": [
    "# Convert tonumpy\n",
    "x_train = np.array(x_values, dtype=np.float32) # only 1-d \n",
    "x_train.shape\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 6.]\n",
      " [ 7.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [10.]]\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: 2drequired\n",
    "x_train = x_train.reshape(-1,1)\n",
    "x_train.shape\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ y = 2x +1 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_values = [2*i + 1 for i in x_values] # building relationship for y\n",
    "y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you're weak in list iterators\n",
    "y_values = []\n",
    "for i in x_values:\n",
    "    result = 2*i + 1\n",
    "    y_values.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  3.  5.  7.  9. 11. 13. 15. 17. 19. 21.]\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(y_values, dtype=np.float32) # again convert to NumPy\n",
    "y_train.shape\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.]\n",
      " [ 3.]\n",
      " [ 5.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [11.]\n",
      " [13.]\n",
      " [15.]\n",
      " [17.]\n",
      " [19.]\n",
      " [21.]]\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: 2D required\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_train.shape\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Building Model\n",
    "##### Critical imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # require for linear regression model\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Model\n",
    "\n",
    "We want to model this relationship given a particular x dataset\n",
    "1. Linear Model\n",
    "    - True Equation: y = 2x + 1\n",
    "2. Forward\n",
    "    - Example\n",
    "        - Input x = 1\n",
    "        - Output y = ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model class\n",
    "class LinearRegressionModel(nn.Module): # passing nn.Module <- always when define modul\n",
    "    def __init__(self, input_size, output_size): \n",
    "        super(LinearRegressionModel, self).__init__() # super () <- inherits from nn.Module and allows \n",
    "        # you to use verything from nn.Module\n",
    "        self.linear = nn.Linear(input_dim, output_dim) # input dim = x, output dim = y\n",
    "    \n",
    "    def forward(self,x): #  x -values of x\n",
    "        out = self.linear(x) # self.linear, we call this function for each value of x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instantiate Model Class\n",
    "    - input: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    - desired output: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "model = LinearRegressionModel(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Loss Class\n",
    "- How we want to minimize the distance between the predicted values and true values\n",
    "- MSE Loss: Mean Squared Error \n",
    "- MSE = $ \\frac{1}{n} \\sum_{i = 1}^{n} (\\hat{y}_{i} - y_{i})^2 $\n",
    "    - $ \\hat{y} $ : prediction\n",
    "    - $ y $ : true value\n",
    "- Basicaly it counts true value and predicted value deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This basically measures the distance (the mean square error)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Optimizer Class\n",
    "\n",
    "- Simplified equation\n",
    "    - $ \\theta = \\theta - \\eta * \\nabla_{\\theta} $\n",
    "        - $ \\theta $ : parameters (our variables, we want to continually update them - $ \\alpha $ and $ \\beta $)\n",
    "        - $ \\eta $ : learning rate (how fast we want to learn)\n",
    "        - $ \\nabla_{\\theta} $ : parameters' gradients\n",
    "- Even simplier equation\n",
    "    - <b> parameters = parameters - learning_rate * parameters_gradients </b>\n",
    "        - parameters: $ \\alpha $ and $ \\beta $ in $ y = \\alpha x + \\beta $\n",
    "        - We want to update parameters $\\alpha$ and $\\beta$ to fit the line as best as possible to the points we have\n",
    "        - desired parameters: $ \\alpha = 2 $ and $ \\beta = 1 $ in $ y = 2x +1 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# think about the learning rate like how fast we want our model to \n",
    "# do this learning proccess to get the best fitting graph the line\n",
    "# We dont want big value because we dont want graph to be movin up and down too much\n",
    "# We want to make gradual changes to reach its ideal final destination (local minima)\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Instiantiate Optimizer Class\n",
    "# SGD is method of optimalization\n",
    "# We minimize the distance between the predicted and true values\n",
    "# model paramaters <- alpha & beta\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n",
    "- 1 epoch: going through the whole x_train data once\n",
    "    - 100 epochs:\n",
    "        - 100x mapping  <b> x_train = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] </b>\n",
    "- Process\n",
    "    1. Convert inputs/labels tovariables\n",
    "    2. Clear gradient buffets\n",
    "    3. Get output given inputs\n",
    "    4. Get loss\n",
    "    5. Get gradients w.r.t. (without respect to) parameters\n",
    "    6. Update parameters using gradients\n",
    "        - <b> parameters = parameters - learning_rate * parameters_gradients </b>\n",
    "    7. REPEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you do more epochs your final parameters would more accurately measure the values that we want \n",
    "# Be awared if you train for too many epochs !\n",
    "# It will work on train data not on test data\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 57.81561279296875\n",
      "epoch 2, loss 5.415572166442871\n",
      "epoch 3, loss 1.133652925491333\n",
      "epoch 4, loss 0.7766638994216919\n",
      "epoch 5, loss 0.7399051189422607\n",
      "epoch 6, loss 0.7293522357940674\n",
      "epoch 7, loss 0.7210208177566528\n",
      "epoch 8, loss 0.7129541635513306\n",
      "epoch 9, loss 0.704991340637207\n",
      "epoch 10, loss 0.6971186399459839\n",
      "epoch 11, loss 0.6893337368965149\n",
      "epoch 12, loss 0.6816362142562866\n",
      "epoch 13, loss 0.6740244030952454\n",
      "epoch 14, loss 0.6664975881576538\n",
      "epoch 15, loss 0.6590554714202881\n",
      "epoch 16, loss 0.6516958475112915\n",
      "epoch 17, loss 0.6444182395935059\n",
      "epoch 18, loss 0.6372219324111938\n",
      "epoch 19, loss 0.6301064491271973\n",
      "epoch 20, loss 0.6230700016021729\n",
      "epoch 21, loss 0.6161119937896729\n",
      "epoch 22, loss 0.6092323064804077\n",
      "epoch 23, loss 0.6024290323257446\n",
      "epoch 24, loss 0.5957015156745911\n",
      "epoch 25, loss 0.5890496969223022\n",
      "epoch 26, loss 0.582471489906311\n",
      "epoch 27, loss 0.5759672522544861\n",
      "epoch 28, loss 0.5695356726646423\n",
      "epoch 29, loss 0.5631757378578186\n",
      "epoch 30, loss 0.5568865537643433\n",
      "epoch 31, loss 0.550667941570282\n",
      "epoch 32, loss 0.5445188283920288\n",
      "epoch 33, loss 0.5384387373924255\n",
      "epoch 34, loss 0.5324258804321289\n",
      "epoch 35, loss 0.5264801979064941\n",
      "epoch 36, loss 0.5206012725830078\n",
      "epoch 37, loss 0.5147876739501953\n",
      "epoch 38, loss 0.5090389847755432\n",
      "epoch 39, loss 0.5033547878265381\n",
      "epoch 40, loss 0.49773383140563965\n",
      "epoch 41, loss 0.4921756982803345\n",
      "epoch 42, loss 0.4866797924041748\n",
      "epoch 43, loss 0.4812452495098114\n",
      "epoch 44, loss 0.475870817899704\n",
      "epoch 45, loss 0.47055697441101074\n",
      "epoch 46, loss 0.46530210971832275\n",
      "epoch 47, loss 0.4601064622402191\n",
      "epoch 48, loss 0.4549683630466461\n",
      "epoch 49, loss 0.44988787174224854\n",
      "epoch 50, loss 0.44486427307128906\n",
      "epoch 51, loss 0.43989622592926025\n",
      "epoch 52, loss 0.43498390913009644\n",
      "epoch 53, loss 0.4301266372203827\n",
      "epoch 54, loss 0.4253237247467041\n",
      "epoch 55, loss 0.42057403922080994\n",
      "epoch 56, loss 0.41587749123573303\n",
      "epoch 57, loss 0.41123372316360474\n",
      "epoch 58, loss 0.4066412150859833\n",
      "epoch 59, loss 0.40210047364234924\n",
      "epoch 60, loss 0.39761024713516235\n",
      "epoch 61, loss 0.393170028924942\n",
      "epoch 62, loss 0.38877955079078674\n",
      "epoch 63, loss 0.38443800806999207\n",
      "epoch 64, loss 0.38014522194862366\n",
      "epoch 65, loss 0.3759002387523651\n",
      "epoch 66, loss 0.3717026114463806\n",
      "epoch 67, loss 0.3675519526004791\n",
      "epoch 68, loss 0.3634476065635681\n",
      "epoch 69, loss 0.35938894748687744\n",
      "epoch 70, loss 0.355375736951828\n",
      "epoch 71, loss 0.35140711069107056\n",
      "epoch 72, loss 0.347483366727829\n",
      "epoch 73, loss 0.34360283613204956\n",
      "epoch 74, loss 0.3397659361362457\n",
      "epoch 75, loss 0.33597180247306824\n",
      "epoch 76, loss 0.33222025632858276\n",
      "epoch 77, loss 0.3285102844238281\n",
      "epoch 78, loss 0.32484155893325806\n",
      "epoch 79, loss 0.3212142586708069\n",
      "epoch 80, loss 0.3176272213459015\n",
      "epoch 81, loss 0.314080148935318\n",
      "epoch 82, loss 0.31057319045066833\n",
      "epoch 83, loss 0.30710482597351074\n",
      "epoch 84, loss 0.30367550253868103\n",
      "epoch 85, loss 0.30028456449508667\n",
      "epoch 86, loss 0.2969314157962799\n",
      "epoch 87, loss 0.29361554980278015\n",
      "epoch 88, loss 0.29033681750297546\n",
      "epoch 89, loss 0.2870945632457733\n",
      "epoch 90, loss 0.283888578414917\n",
      "epoch 91, loss 0.28071847558021545\n",
      "epoch 92, loss 0.27758389711380005\n",
      "epoch 93, loss 0.27448412775993347\n",
      "epoch 94, loss 0.27141889929771423\n",
      "epoch 95, loss 0.2683880031108856\n",
      "epoch 96, loss 0.26539117097854614\n",
      "epoch 97, loss 0.2624274790287018\n",
      "epoch 98, loss 0.2594969868659973\n",
      "epoch 99, loss 0.2565990686416626\n",
      "epoch 100, loss 0.2537338137626648\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # we run \"epoch 1 not 0\"\n",
    "    epoch += 1\n",
    "    \n",
    "    # Convert numpy array to torch -> then ->\n",
    "    # -> to Variable that can accumulate gradiatents\n",
    "    inputs = Variable(torch.from_numpy(x_train))\n",
    "    labels = Variable(torch.from_numpy(y_train))\n",
    "    \n",
    "    # Clear gradients w.r.t. parameters <- for every epoch, we dont want to \n",
    "    # accumulate the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward to get output\n",
    "    # Outputs <- y (certain values)\n",
    "    # Inputs <- x\n",
    "    # Model <- multiplying by alfa * input + beta\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Calculate scalar Loss\n",
    "    # We compare our predicted values to real values\n",
    "    # to minimize the loss via function before\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Getting gradients w.r.t. parameters\n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Upating parameters\n",
    "    # Again in next epoch we clear the optimizer\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('epoch {}, loss {}'.format(epoch, loss))\n",
    "    # We see that predicted values are far away from true y values\n",
    "    # Error is huge -> Backpropagation gets this error and update the parameters ->\n",
    "    # smaller loss -> we get gradients -> update parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06297887],\n",
       "       [ 2.1979184 ],\n",
       "       [ 4.3328576 ],\n",
       "       [ 6.4677973 ],\n",
       "       [ 8.602736  ],\n",
       "       [10.737676  ],\n",
       "       [12.872616  ],\n",
       "       [15.007555  ],\n",
       "       [17.142494  ],\n",
       "       [19.277433  ],\n",
       "       [21.412373  ]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Purely inference\n",
    "# We are passing to our model, tensor variable of x train data\n",
    "# We want to access variable so .data then .numpy to be more readable \n",
    "predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "# Print predicted y\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 7.],\n",
       "       [ 9.],\n",
       "       [11.],\n",
       "       [13.],\n",
       "       [15.],\n",
       "       [17.],\n",
       "       [19.],\n",
       "       [21.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y = 2x + 1\n",
    "y_train\n",
    "# Predictive values are relatively close to our labels\n",
    "# We start to realize that after training we have parameters that can\n",
    "# predict the values which are idealy identical or close to our labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clear figure\n",
    "plt.clf()\n",
    "\n",
    "# Get predicitions\n",
    "# We feed x data to our model\n",
    "predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "\n",
    "# Plot true data\n",
    "plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n",
    "\n",
    "# Plot predicitions\n",
    "plt.plot(x_train, predicted, '--', label='Predictions', alpha=0.5)\n",
    "\n",
    "# Legend and plot\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False\n",
    "if save_model is True:\n",
    "    # Save only parameters - alpha(coefficient) & beta(intercept)\n",
    "    torch.save(model.state_dict(), 'awesome_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "if load_model is True:\n",
    "    model.load_state_dict(torch.load('awesome_model.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Code summary\n",
    "GPU: 2 things must be on GPU\n",
    "- model\n",
    "- variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1, loss 105.61937713623047\n",
      "epoch2, loss 9.099535942077637\n",
      "epoch3, loss 1.221309781074524\n",
      "epoch4, loss 0.5733572840690613\n",
      "epoch5, loss 0.5152159333229065\n",
      "epoch6, loss 0.5052423477172852\n",
      "epoch7, loss 0.4992563724517822\n",
      "epoch8, loss 0.4936527609825134\n",
      "epoch9, loss 0.4881381392478943\n",
      "epoch10, loss 0.4826871156692505\n",
      "epoch11, loss 0.47729673981666565\n",
      "epoch12, loss 0.47196701169013977\n",
      "epoch13, loss 0.46669644117355347\n",
      "epoch14, loss 0.4614847004413605\n",
      "epoch15, loss 0.45633161067962646\n",
      "epoch16, loss 0.45123571157455444\n",
      "epoch17, loss 0.44619685411453247\n",
      "epoch18, loss 0.4412141740322113\n",
      "epoch19, loss 0.4362873435020447\n",
      "epoch20, loss 0.4314153790473938\n",
      "epoch21, loss 0.4265977442264557\n",
      "epoch22, loss 0.42183423042297363\n",
      "epoch23, loss 0.4171236753463745\n",
      "epoch24, loss 0.4124656319618225\n",
      "epoch25, loss 0.407859742641449\n",
      "epoch26, loss 0.40330514311790466\n",
      "epoch27, loss 0.3988012373447418\n",
      "epoch28, loss 0.39434814453125\n",
      "epoch29, loss 0.38994452357292175\n",
      "epoch30, loss 0.385590136051178\n",
      "epoch31, loss 0.38128429651260376\n",
      "epoch32, loss 0.37702658772468567\n",
      "epoch33, loss 0.3728165328502655\n",
      "epoch34, loss 0.368653267621994\n",
      "epoch35, loss 0.36453646421432495\n",
      "epoch36, loss 0.3604656159877777\n",
      "epoch37, loss 0.3564402163028717\n",
      "epoch38, loss 0.35246017575263977\n",
      "epoch39, loss 0.34852421283721924\n",
      "epoch40, loss 0.3446323871612549\n",
      "epoch41, loss 0.34078389406204224\n",
      "epoch42, loss 0.3369787335395813\n",
      "epoch43, loss 0.33321526646614075\n",
      "epoch44, loss 0.32949429750442505\n",
      "epoch45, loss 0.325814813375473\n",
      "epoch46, loss 0.32217687368392944\n",
      "epoch47, loss 0.3185790479183197\n",
      "epoch48, loss 0.31502169370651245\n",
      "epoch49, loss 0.31150397658348083\n",
      "epoch50, loss 0.3080254793167114\n",
      "epoch51, loss 0.3045855164527893\n",
      "epoch52, loss 0.30118435621261597\n",
      "epoch53, loss 0.29782119393348694\n",
      "epoch54, loss 0.29449552297592163\n",
      "epoch55, loss 0.2912067770957947\n",
      "epoch56, loss 0.2879549264907837\n",
      "epoch57, loss 0.28473928570747375\n",
      "epoch58, loss 0.28155964612960815\n",
      "epoch59, loss 0.27841559052467346\n",
      "epoch60, loss 0.2753066122531891\n",
      "epoch61, loss 0.2722321152687073\n",
      "epoch62, loss 0.26919224858283997\n",
      "epoch63, loss 0.2661861181259155\n",
      "epoch64, loss 0.2632139325141907\n",
      "epoch65, loss 0.2602744996547699\n",
      "epoch66, loss 0.2573677599430084\n",
      "epoch67, loss 0.25449392199516296\n",
      "epoch68, loss 0.25165218114852905\n",
      "epoch69, loss 0.24884183704853058\n",
      "epoch70, loss 0.24606329202651978\n",
      "epoch71, loss 0.24331551790237427\n",
      "epoch72, loss 0.24059860408306122\n",
      "epoch73, loss 0.23791159689426422\n",
      "epoch74, loss 0.23525512218475342\n",
      "epoch75, loss 0.23262779414653778\n",
      "epoch76, loss 0.23003025352954865\n",
      "epoch77, loss 0.22746159136295319\n",
      "epoch78, loss 0.22492161393165588\n",
      "epoch79, loss 0.22240976989269257\n",
      "epoch80, loss 0.21992623805999756\n",
      "epoch81, loss 0.21747033298015594\n",
      "epoch82, loss 0.215041846036911\n",
      "epoch83, loss 0.21264031529426575\n",
      "epoch84, loss 0.21026606857776642\n",
      "epoch85, loss 0.20791803300380707\n",
      "epoch86, loss 0.20559610426425934\n",
      "epoch87, loss 0.20330044627189636\n",
      "epoch88, loss 0.20103009045124054\n",
      "epoch89, loss 0.198785200715065\n",
      "epoch90, loss 0.19656534492969513\n",
      "epoch91, loss 0.1943703144788742\n",
      "epoch92, loss 0.19219987094402313\n",
      "epoch93, loss 0.1900538057088852\n",
      "epoch94, loss 0.18793129920959473\n",
      "epoch95, loss 0.185832679271698\n",
      "epoch96, loss 0.1837574988603592\n",
      "epoch97, loss 0.18170543015003204\n",
      "epoch98, loss 0.17967639863491058\n",
      "epoch99, loss 0.17766998708248138\n",
      "epoch100, loss 0.17568621039390564\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "STEP 1: CREATE MODEL CLASS\n",
    "'''\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "    \n",
    "'''\n",
    "STEP 2: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "model = LinearRegressionModel(input_dim, output_dim)\n",
    "\n",
    "#########################\n",
    "### USE GPU FOR MODEL ###\n",
    "#########################\n",
    "\n",
    "# if we can compile code on gpu\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "''' \n",
    "STEP 3: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "'''\n",
    "STEP 4: INSTASNTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 5: TRAIN THE MODEL\n",
    "'''\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    # Convert numpy array to torch Variable\n",
    "    \n",
    "    #########################\n",
    "    ### USE GPU FOR MODEL ###\n",
    "    #########################\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "    else:\n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        labels = Variable(torch.from_numpy(y_train))\n",
    "        \n",
    "    # Clear gradients w.r.t. parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward to get output\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Getting gradients w.r.t. parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Updating parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # logging\n",
    "    \n",
    "    print('epoch{}, loss {}'.format(epoch, loss.data))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- Simple <b> linear regression basics </b>\n",
    "    - $ y = \\alpha x + \\beta $\n",
    "    - $ y = 2x + 1 $\n",
    "- <b> Example </b> of simple linear regression\n",
    "- <b> Aim </b> of linear regression\n",
    "    - Minimizing distance between the points and the line\n",
    "        - Calculate \"distance\" through MSE\n",
    "        - Calculate gradients\n",
    "        - Update parameters with <b> parameters = parameters - learning_rate*gradients\n",
    "        - Slowly update parameters $A$ and $B$ model the linear relationship between $y$ and $x$ of the form $ y = 2x + 1 $\n",
    "    - Built a linear regression <b> model </b> in <b> CPU </b> and <b> GPU </b>\n",
    "        - Step 1: Create Model Class\n",
    "        - Step 2: Instantiate Model Class\n",
    "        - Step 3: Instantiate Loss Class\n",
    "        - Step 4: Instantiate Optimizer Class\n",
    "        - Step 5: Train Model\n",
    "    - Important things to be on <b> GPU </b>\n",
    "        - model\n",
    "        - variables\n",
    "    - How to bring to <b> GPU </b> ?\n",
    "        - model_name.cuda()\n",
    "        - variable_name.cuda()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
