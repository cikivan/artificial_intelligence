{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regresion / Logistic Regression problems\n",
    "- Can represent linear functions well\n",
    "    - $ y = 2x + 3 $\n",
    "    - $ y = x_{1} + x_{2} $\n",
    "    - $ y = x_{1} + 3x_{2} + 4x_{3} $\n",
    "- Cannot represent <b> non-linear </b> functions \n",
    "    - $ y = 4x_{1} + 2x_{2}^2 + 3x_{3}^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Introducing a Non-linear Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Logistic Regression we have:\n",
    "    - <b> Input </b> - > (Linear Function) <b>Logits </b>-> (Softmax Function) <b>Softmax </b>-> (Cross Entropy Function) <b>Labels</b>\n",
    "\n",
    "- In 1 Layer Neural Network we have:\n",
    "    - Input Layer : Input\n",
    "    - 1 Hidden Layer : Linear Function, Logits, Non Linear Function, Non Linear Output\n",
    "    - Readout Layer : Logits , Softmax Function, Softmax\n",
    "    - Cross Entropy Function, Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nonlinear Functions:\n",
    "    - ReLUS\n",
    "    - Sigmoid\n",
    "    - Tanh\n",
    "    \n",
    "#### Sigmoid ( Logistic )\n",
    "- $ \\sigma (x) = \\frac{1}{1+e^{-x}} $\n",
    "- Input number -> output is between [0,1]\n",
    "    - Large negative number -> 0\n",
    "    - Large positive number -> 1\n",
    "- Cons:\n",
    "    - Activation saturates at 0 or 1 with gradients ~ 0\n",
    "        - No signal to update weights -> cannot learn\n",
    "        - Solution: Have to carefully initialize weights to prevent this\n",
    "    - Outputs not centerend around 0\n",
    "        - If output always positive -> gradients always positive or negative -> bad for gradients updates\n",
    "        \n",
    "#### TanH\n",
    "- $ tanh(x) = 2\\sigma(2x) - 1 $\n",
    "    - A scaled sigmoid function\n",
    "- Input number -> [-1,1]\n",
    "- Cons:\n",
    "    - Activation saturates at 0 or 1 with gradients ~ 0\n",
    "        - No signal to update weights -> cannot learn\n",
    "        - Solution: Have to carefully initialize weights to prevent this\n",
    "        \n",
    "#### ReLUs\n",
    "- $ f(x) = max(0,x) $\n",
    "- Pros:\n",
    "    - Accelerates convergence -> train faster -> Less iterations then others\n",
    "    - Less computationally expensive operation compared to Sigmoid/Tanh exponentials\n",
    "- Cons:\n",
    "    - Many ReLU units \"die\" -> gradients = 0 forever\n",
    "    - Solution: careful learning rate choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a FeedForward Neural Network with Pytorch\n",
    "#### Model A: 1 Hidden Layer Feedforward Neural Network (Sigmoid Activation)\n",
    "\n",
    "Steps:\n",
    "- Step 1: Load Dataset\n",
    "- Step 2: Make Dataset Iterable\n",
    "- Step 3: Create Model Class\n",
    "- Step 4: Instantiate Model Class\n",
    "- Step 5: Instantiate Loss Class\n",
    "- Step 6: Instantiate Optimizer Class\n",
    "- Step 7: Train Model\n",
    "\n",
    "#### Step 1: Loading MNIST Train Dataset\n",
    "\n",
    "<b> Images from 1 to 9</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "test_dataset = dsets.MNIST(root='./data',\n",
    "                               train=False,\n",
    "                               transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Make Dataset Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100 # 100 images at time\n",
    "n_iters = 3000 # 1 epochs = number of iterations\n",
    "num_epochs = n_iters / (len(train_dataset)/ batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Create Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(nn.Module): # pyTorch stuff\n",
    "    def __init__(self,input_size,hidden_size,num_classes): # hiden_size - nonliearity dimension\n",
    "        super(FeedforwardNeuralNetModel,self).__init__()\n",
    "        #Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) # Input - > hidden\n",
    "        #Non-linearity\n",
    "        self.sigmoid = nn.Sigmoid() # activation function\n",
    "        # Linear function(readout) -> hidden -> output\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "# running functions\n",
    "    def forward(self,x):\n",
    "        # LINEAR \n",
    "        out = self.fc1(x)\n",
    "        # NON-LINEAR       \n",
    "        out = self.sigmoid(out)\n",
    "        # LINEAR - readout\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Instantiate Model Class\n",
    "- <b> Input </b> dimension: <b> 784</b>\n",
    "    - Size of image : $ 28x28 = 784 $\n",
    "- <b> Output </b> dimension: <b> 10 </b>\n",
    "    - 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
    "- <b> Hidden </b> dimension: <b> 100 </b>\n",
    "    - Can be any number\n",
    "    - Similar term\n",
    "        - Number of neurons\n",
    "        - Number of non-linear activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Instantiate Loss Class\n",
    "\n",
    "- Feedforward Neural Network: <b> Cross Entropy Loss </b>\n",
    "    - Logistic Regression: <b> Cross Entropy Loss </b>\n",
    "    - Linear Regression : <b> MSE </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 6: Instatiate Optimizer Class\n",
    "- Simplified equation\n",
    "    - $ \\theta = \\theta - \\eta * \\nabla_{\\theta} $\n",
    "    - $ \\theta $ : parameters (our variables)\n",
    "    - $ \\eta $ : learning rate (how fast we want to learn)\n",
    "    - $ \\nabla_{\\theta}$ : paramterers' gradients\n",
    "- Even simplifier equation\n",
    "    - $ parameters = parameters - learning\\_rate * parameter\\_gradients $\n",
    "    - <b> At every iteration(batch_size), we update our model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters In-Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x7f1451bd2678>\n",
      "4\n",
      "torch.Size([100, 784])\n",
      "torch.Size([100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())\n",
    "\n",
    "print(len(list(model.parameters())))\n",
    "\n",
    "# Hidden Layer Parameters\n",
    "print(list(model.parameters())[0].size())\n",
    "\n",
    "# FC 1 Bias Parameters\n",
    "print(list(model.parameters())[1].size())\n",
    "\n",
    "# FC 2 Parameters\n",
    "print(list(model.parameters())[2].size())\n",
    "\n",
    "# FC 2 Bias Parameters\n",
    "print(list(model.parameters())[3].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Train Model\n",
    "- Process\n",
    "    - 1. Conver inputs / labels to variables\n",
    "    - 2. Clearn gradient buffers\n",
    "    - 3. Get output given inputs\n",
    "    - 4. Get loss\n",
    "    - 5. Get gradient w.r.t. parameters\n",
    "    - 6. Update parameters using gradients\n",
    "        - $ parameters = parameters - learning\\_rate * parameters\\_gradients$\n",
    "    - 7. REPEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500, Loss: 0.6876333355903625, Accuracy:86\n",
      "Iteration: 1000, Loss: 0.500399649143219, Accuracy:89\n",
      "Iteration: 1500, Loss: 0.5672891736030579, Accuracy:90\n",
      "Iteration: 2000, Loss: 0.3102739453315735, Accuracy:91\n",
      "Iteration: 2500, Loss: 0.20339469611644745, Accuracy:91\n",
      "Iteration: 3000, Loss: 0.3302624523639679, Accuracy:91\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "# Now 5 epochs in this case\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # 60k images loaded 100 by 100 <- batch_size\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # 1 ITERATION = 1 CYCLE\n",
    "        # Load images as Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        # Don't want to accumulate gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter +=1\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterats through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = Variable(images.view(-1,28*28))\n",
    "            \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data,1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correcnt predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "        \n",
    "            ## Print loss\n",
    "            print('Iteration: {}, Loss: {}, Accuracy:{}'.format(iter,loss.data,accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model B: 1 Hidden Layer Feedforward Neural Network (Tanh Activation)\n",
    "\n",
    "Steps:\n",
    "- Step 1: Load Dataset\n",
    "- Step 2: Make Dataset Iterable\n",
    "- <b> Step 3: Create Model Class </b> - We will make changes in this step - changing the activation function\n",
    "- Step 4: Instantiate Model Class\n",
    "- Step 5: Instantiate Loss Class\n",
    "- Step 6: Instantiate Optimizer Class\n",
    "- Step 7: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500, Loss: 0.3404415249824524, Accuracy:90\n",
      "Iteration: 1000, Loss: 0.34508755803108215, Accuracy:92\n",
      "Iteration: 1500, Loss: 0.4050217568874359, Accuracy:93\n",
      "Iteration: 2000, Loss: 0.1702473759651184, Accuracy:93\n",
      "Iteration: 2500, Loss: 0.12349386513233185, Accuracy:94\n",
      "Iteration: 3000, Loss: 0.2238638550043106, Accuracy:95\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "batch_size = 100 # 100 images at time\n",
    "n_iters = 3000 # 1 epochs = number of iterations\n",
    "num_epochs = n_iters / (len(train_dataset)/ batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "test_dataset = dsets.MNIST(root='./data',\n",
    "                               train=False,\n",
    "                               transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100 # 100 images at time\n",
    "n_iters = 3000 # 1 epochs = number of iterations\n",
    "num_epochs = n_iters / (len(train_dataset)/ batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "\n",
    "class FeedforwardNeuralNetModel(nn.Module): # pyTorch stuff\n",
    "    def __init__(self,input_size,hidden_size,num_classes): # hiden_size - nonliearity dimension\n",
    "        super(FeedforwardNeuralNetModel,self).__init__()\n",
    "        #Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) # Input - > hidden\n",
    "        #Non-linearity\n",
    "        self.tanh = nn.Tanh() ################################ ! Here is the change to tanh activation function\n",
    "        # Linear function(readout) -> hidden -> output\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "# running functions\n",
    "    def forward(self,x):\n",
    "        # LINEAR \n",
    "        out = self.fc1(x)\n",
    "        # NON-LINEAR       \n",
    "        out = self.tanh(out) ################################ ! Here is the change to tanh activation function\n",
    "        # LINEAR - readout\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN MODEL\n",
    "'''\n",
    "iter = 0\n",
    "# Now 5 epochs in this case\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # 60k images loaded 100 by 100 <- batch_size\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # 1 ITERATION = 1 CYCLE\n",
    "        # Load images as Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        # Don't want to accumulate gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter +=1\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterats through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = Variable(images.view(-1,28*28))\n",
    "            \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data,1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correcnt predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "        \n",
    "            ## Print loss\n",
    "            print('Iteration: {}, Loss: {}, Accuracy:{}'.format(iter,loss.data,accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model C: 1 Hidden Layer Feedforward Neural Network (ReLU Activation)\n",
    "\n",
    "Steps:\n",
    "- Step 1: Load Dataset\n",
    "- Step 2: Make Dataset Iterable\n",
    "- <b> Step 3: Create Model Class </b> - We will make changes in this step - changing the activation function\n",
    "- Step 4: Instantiate Model Class\n",
    "- Step 5: Instantiate Loss Class\n",
    "- Step 6: Instantiate Optimizer Class\n",
    "- Step 7: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500, Loss: 0.31928738951683044, Accuracy:90\n",
      "Iteration: 1000, Loss: 0.3033466041088104, Accuracy:93\n",
      "Iteration: 1500, Loss: 0.36112144589424133, Accuracy:93\n",
      "Iteration: 2000, Loss: 0.16185984015464783, Accuracy:94\n",
      "Iteration: 2500, Loss: 0.11418130993843079, Accuracy:95\n",
      "Iteration: 3000, Loss: 0.23605427145957947, Accuracy:95\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "batch_size = 100 # 100 images at time\n",
    "n_iters = 3000 # 1 epochs = number of iterations\n",
    "num_epochs = n_iters / (len(train_dataset)/ batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "test_dataset = dsets.MNIST(root='./data',\n",
    "                               train=False,\n",
    "                               transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100 # 100 images at time\n",
    "n_iters = 3000 # 1 epochs = number of iterations\n",
    "num_epochs = n_iters / (len(train_dataset)/ batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "\n",
    "class FeedforwardNeuralNetModel(nn.Module): # pyTorch stuff\n",
    "    def __init__(self,input_size,hidden_size,num_classes): # hiden_size - nonliearity dimension\n",
    "        super(FeedforwardNeuralNetModel,self).__init__()\n",
    "        #Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) # Input - > hidden\n",
    "        #Non-linearity\n",
    "        self.relu = nn.ReLU() ################################ ! Here is the change to tanh activation function\n",
    "        # Linear function(readout) -> hidden -> output\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "# running functions\n",
    "    def forward(self,x):\n",
    "        # LINEAR \n",
    "        out = self.fc1(x)\n",
    "        # NON-LINEAR       \n",
    "        out = self.relu(out) ################################ ! Here is the change to tanh activation function\n",
    "        # LINEAR - readout\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN MODEL\n",
    "'''\n",
    "iter = 0\n",
    "# Now 5 epochs in this case\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # 60k images loaded 100 by 100 <- batch_size\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # 1 ITERATION = 1 CYCLE\n",
    "        # Load images as Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        # Don't want to accumulate gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter +=1\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterats through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = Variable(images.view(-1,28*28))\n",
    "            \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data,1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correcnt predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "        \n",
    "            ## Print loss\n",
    "            print('Iteration: {}, Loss: {}, Accuracy:{}'.format(iter,loss.data,accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model D: 2 Hidden Layer Feedforward Neural Network (ReLU Activation)\n",
    "\n",
    "Steps:\n",
    "- Step 1: Load Dataset\n",
    "- Step 2: Make Dataset Iterable\n",
    "- <b> Step 3: Create Model Class </b> - We will make changes in this step - adding 1 more layer\n",
    "- Step 4: Instantiate Model Class\n",
    "- Step 5: Instantiate Loss Class\n",
    "- Step 6: Instantiate Optimizer Class\n",
    "- Step 7: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500, Loss: 0.24639907479286194, Accuracy:90\n",
      "Iteration: 1000, Loss: 0.28348368406295776, Accuracy:93\n",
      "Iteration: 1500, Loss: 0.30106884241104126, Accuracy:94\n",
      "Iteration: 2000, Loss: 0.12275117635726929, Accuracy:95\n",
      "Iteration: 2500, Loss: 0.06751161813735962, Accuracy:95\n",
      "Iteration: 3000, Loss: 0.2389032393693924, Accuracy:96\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "test_dataset = dsets.MNIST(root='./data',\n",
    "                               train=False,\n",
    "                               transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100 # 100 images at time\n",
    "n_iters = 3000 # 1 epochs = number of iterations\n",
    "num_epochs = n_iters / (len(train_dataset)/ batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "\n",
    "class FeedforwardNeuralNetModel(nn.Module): # pyTorch stuff\n",
    "    def __init__(self,input_size,hidden_size,num_classes): # hiden_size - nonliearity dimension\n",
    "        super(FeedforwardNeuralNetModel,self).__init__()\n",
    "       \n",
    "        # Linear function 1: 784 --> 100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) # Input - > hidden        \n",
    "        # Non-linearity 1\n",
    "        self.relu1 = nn.ReLU() \n",
    "        \n",
    "        #Linear fuction 2: 100-> 100\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)   ###### ADDING 1 more hidden layer !!!!!!\n",
    "        #Non-linearity2\n",
    "        self.relu2= nn.ReLU()             ###### ADDING 1 more hidden layer !!!!!!\n",
    "        \n",
    "        # Linear function(readout) 3-> hidden -> output --> 100 -> 10\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "# running functions\n",
    "    def forward(self,x):\n",
    "        # LINEAR f1\n",
    "        out = self.fc1(x)\n",
    "        # NON-LINEAR       \n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # LINEAR f2\n",
    "        out = self.fc2(out)\n",
    "        # NON-LINEAR       \n",
    "        out = self.relu2(out)        \n",
    "        \n",
    "        # LINEAR - readout\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN MODEL\n",
    "'''\n",
    "iter = 0\n",
    "# Now 5 epochs in this case\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # 60k images loaded 100 by 100 <- batch_size\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # 1 ITERATION = 1 CYCLE\n",
    "        # Load images as Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        # Don't want to accumulate gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter +=1\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterats through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = Variable(images.view(-1,28*28))\n",
    "            \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data,1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correcnt predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "        \n",
    "            ## Print loss\n",
    "            print('Iteration: {}, Loss: {}, Accuracy:{}'.format(iter,loss.data,accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model D: 3 Hidden Layer Feedforward Neural Network (ReLU Activation)\n",
    "\n",
    "Steps:\n",
    "- Step 1: Load Dataset\n",
    "- Step 2: Make Dataset Iterable\n",
    "- <b> Step 3: Create Model Class </b> - We will make changes in this step - adding 1 more layer\n",
    "- Step 4: Instantiate Model Class\n",
    "- Step 5: Instantiate Loss Class\n",
    "- Step 6: Instantiate Optimizer Class\n",
    "- Step 7: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500, Loss: 0.2807856500148773, Accuracy:89\n",
      "Iteration: 1000, Loss: 0.2270146906375885, Accuracy:93\n",
      "Iteration: 1500, Loss: 0.2654840350151062, Accuracy:94\n",
      "Iteration: 2000, Loss: 0.0961790457367897, Accuracy:96\n",
      "Iteration: 2500, Loss: 0.04602858051657677, Accuracy:96\n",
      "Iteration: 3000, Loss: 0.20235863327980042, Accuracy:96\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "test_dataset = dsets.MNIST(root='./data',\n",
    "                               train=False,\n",
    "                               transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100 # 100 images at time\n",
    "n_iters = 3000 # 1 epochs = number of iterations\n",
    "num_epochs = n_iters / (len(train_dataset)/ batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "\n",
    "class FeedforwardNeuralNetModel(nn.Module): # pyTorch stuff\n",
    "    def __init__(self,input_size,hidden_size,num_classes): # hiden_size - nonliearity dimension\n",
    "        super(FeedforwardNeuralNetModel,self).__init__()\n",
    "       \n",
    "        # Linear function 1: 784 --> 100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)      \n",
    "        # Non-linearity 1\n",
    "        self.relu1 = nn.ReLU() \n",
    "        \n",
    "        #Linear fuction 2: 100-> 100\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)   \n",
    "        #Non-linearity2\n",
    "        self.relu2= nn.ReLU()  \n",
    "        \n",
    "        #Linear fuction 3: 100-> 100\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)   ###### ADDING 1 more hidden layer !!!!!!\n",
    "        #Non-linearity3\n",
    "        self.relu3= nn.ReLU()             ###### ADDING 1 more hidden layer !!!!!!\n",
    "        \n",
    "        # Linear function(readout) 3-> hidden -> output --> 100 -> 10\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "# running functions\n",
    "    def forward(self,x):\n",
    "        # LINEAR f1\n",
    "        out = self.fc1(x)\n",
    "        # NON-LINEAR       \n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # LINEAR f2\n",
    "        out = self.fc2(out)\n",
    "        # NON-LINEAR       \n",
    "        out = self.relu2(out)  \n",
    "        \n",
    "        # LINEAR f3\n",
    "        out = self.fc3(out)\n",
    "        # NON-LINEAR       \n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        # LINEAR - readout\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN MODEL\n",
    "'''\n",
    "iter = 0\n",
    "# Now 5 epochs in this case\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # 60k images loaded 100 by 100 <- batch_size\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # 1 ITERATION = 1 CYCLE\n",
    "        # Load images as Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        # Don't want to accumulate gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter +=1\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterats through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = Variable(images.view(-1,28*28))\n",
    "            \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data,1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correcnt predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "        \n",
    "            ## Print loss\n",
    "            print('Iteration: {}, Loss: {}, Accuracy:{}'.format(iter,loss.data,accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning\n",
    "- 2 ways to expand a neural network\n",
    "    - More non-linear activation units (neurons)\n",
    "    - More hidden layers\n",
    "- Cons\n",
    "    - Need a larger dataset\n",
    "        - Curse of dimensionality\n",
    "    - Does not necessarily mean higher accuracy\n",
    "    \n",
    "### 3. Building a Feedforward Neural Network with PyTorch (GPU)\n",
    "- GPU : 2 things must be on GPU\n",
    "    - model\n",
    "    - variables\n",
    "    - you can do it with $.cuda()$ function\n",
    "    - add $.cuda()$ in <b> Step 4</b> and <b> Step 7</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500, Loss: 0.2634340524673462, Accuracy:89\n",
      "Iteration: 1000, Loss: 0.2613889276981354, Accuracy:93\n",
      "Iteration: 1500, Loss: 0.25126415491104126, Accuracy:94\n",
      "Iteration: 2000, Loss: 0.11134683340787888, Accuracy:96\n",
      "Iteration: 2500, Loss: 0.038412827998399734, Accuracy:96\n",
      "Iteration: 3000, Loss: 0.22977958619594574, Accuracy:96\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "test_dataset = dsets.MNIST(root='./data',\n",
    "                               train=False,\n",
    "                               transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100 # 100 images at time\n",
    "n_iters = 3000 # 1 epochs = number of iterations\n",
    "num_epochs = n_iters / (len(train_dataset)/ batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "\n",
    "class FeedforwardNeuralNetModel(nn.Module): # pyTorch stuff\n",
    "    def __init__(self,input_size,hidden_size,num_classes): # hiden_size - nonliearity dimension\n",
    "        super(FeedforwardNeuralNetModel,self).__init__()\n",
    "       \n",
    "        # Linear function 1: 784 --> 100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)      \n",
    "        # Non-linearity 1\n",
    "        self.relu1 = nn.ReLU() \n",
    "        \n",
    "        #Linear fuction 2: 100-> 100\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)   \n",
    "        #Non-linearity2\n",
    "        self.relu2= nn.ReLU()  \n",
    "        \n",
    "        #Linear fuction 3: 100-> 100\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)   ###### ADDING 1 more hidden layer !!!!!!\n",
    "        #Non-linearity3\n",
    "        self.relu3= nn.ReLU()             ###### ADDING 1 more hidden layer !!!!!!\n",
    "        \n",
    "        # Linear function(readout) 3-> hidden -> output --> 100 -> 10\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "# running functions\n",
    "    def forward(self,x):\n",
    "        # LINEAR f1\n",
    "        out = self.fc1(x)\n",
    "        # NON-LINEAR       \n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # LINEAR f2\n",
    "        out = self.fc2(out)\n",
    "        # NON-LINEAR       \n",
    "        out = self.relu2(out)  \n",
    "        \n",
    "        # LINEAR f3\n",
    "        out = self.fc3(out)\n",
    "        # NON-LINEAR       \n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        # LINEAR - readout\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# move model to cuda\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN MODEL\n",
    "'''\n",
    "iter = 0\n",
    "# Now 5 epochs in this case\n",
    "for epoch in range(num_epochs):  \n",
    "    # 60k images loaded 100 by 100 <- batch_size\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # 1 ITERATION = 1 CYCLE\n",
    "        # Load images as Variable\n",
    "        \n",
    "        #####################\n",
    "        # USE GPU FOR MODEL #\n",
    "        #####################\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.view(-1, 28*28).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            images = Variable(images.view(-1, 28*28))\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        # Don't want to accumulate gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter +=1\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterats through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                \n",
    "                #####################\n",
    "                # USE GPU FOR MODEL #\n",
    "                #####################\n",
    "                if torch.cuda.is_available():\n",
    "                        images = Variable(images.view(-1,28*28).cuda())\n",
    "                else:\n",
    "                        images = Variable(images.view(-1,28*28))\n",
    "            \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data,1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                #####################\n",
    "                # USE GPU FOR MODEL #\n",
    "                #####################\n",
    "                # Total correcnt predictions\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "        \n",
    "            ## Print loss\n",
    "            print('Iteration: {}, Loss: {}, Accuracy:{}'.format(iter,loss.data,accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
